import torch
import torch.nn as nn
import torch.nn.functional as F


class PatchLanguageFusion(nn.Module):
    """
    åœ¨ SA2 patch å’Œè¯­è¨€ token ä¹‹é—´åš cross-attentionï¼ˆtoken -> patchï¼‰ï¼Œ
    ä»¥â€œæ®‹å·®å¢å¼ºâ€çš„æ–¹å¼èåˆåˆ°åŸå§‹ lang_fea ä¸Šï¼Œé¿å…ç›´æ¥è¦†ç›–å¯¼è‡´ ref åˆ†æ”¯å´©æ‰ã€‚

    å‡è®¾ï¼š
        data_dict["sa2_features"]: (B_pc, Cpc=256, Np)
        data_dict["lang_fea"]:     (B_L = B_pc * L, T, D_lang)
    """

    def __init__(self,
                 pc_feat_dim=256,        # sa2_features çš„é€šé“æ•°
                 lang_hidden_size=128,   # ä¸ LangBertModule çš„ lang_hidden_size ä¸€è‡´
                 depth=1,
                 num_heads=4):
        super().__init__()

        self.lang_hidden_size = lang_hidden_size
        self.depth = depth
        self.num_heads = num_heads

        # SA2 patch â†’ è¯­è¨€ç»´åº¦
        self.pc_proj = nn.Linear(pc_feat_dim, lang_hidden_size)

        # å¤šå±‚ cross-attn + FFN
        self.layers = nn.ModuleList()
        for _ in range(depth):
            self.layers.append(
                nn.ModuleDict({
                    "attn": nn.MultiheadAttention(
                        embed_dim=lang_hidden_size,
                        num_heads=num_heads,
                        batch_first=True  # è¾“å…¥è¾“å‡º (B, seq, D)
                    ),
                    "ffn": nn.Sequential(
                        nn.Linear(lang_hidden_size, lang_hidden_size * 4),
                        nn.ReLU(inplace=True),
                        nn.Linear(lang_hidden_size * 4, lang_hidden_size)
                    ),
                    "norm1": nn.LayerNorm(lang_hidden_size),
                    "norm2": nn.LayerNorm(lang_hidden_size)
                })
            )

        # ğŸ”‘ å¯å­¦ä¹  gateï¼Œæ§åˆ¶èåˆå¼ºåº¦ï¼Œåˆå§‹åŒ–å¾ˆå°ï¼š
        # sigmoid(-3) â‰ˆ 0.047ï¼Œç›¸å½“äºä¸€å¼€å§‹å‡ ä¹ä¸æ”¹å˜åŸæ¥çš„ lang_fea
        self.gate_logit = nn.Parameter(torch.tensor(-3.0))

    def forward(self, data_dict):

        # å®‰å…¨æ£€æŸ¥
        if ("sa2_features" not in data_dict) or ("lang_fea" not in data_dict):
            return data_dict

        # -------- SA2 patch ç‰¹å¾ --------
        # sa2: (B_pc, 256, Np)
        sa2 = data_dict["sa2_features"]
        B_pc, Cpc, Np = sa2.shape
        # â†’ (B_pc, Np, 256)
        sa2 = sa2.permute(0, 2, 1)

        # -------- è¯­è¨€ç‰¹å¾ --------
        # lang_fea: (B_L, T, D_lang)
        lang_fea = data_dict["lang_fea"]
        B_L, T, D_lang = lang_fea.shape

        assert D_lang == self.lang_hidden_size, \
            f"lang_fea dim = {D_lang}, ä½† PatchLanguageFusion è®¾å®šçš„æ˜¯ {self.lang_hidden_size}"

        # è¿™é‡Œæˆ‘ä»¬åªè¦æ±‚ B_L æ˜¯ B_pc çš„æ•´æ•°å€
        assert B_L % B_pc == 0, f"lang batch {B_L} ä¸æ˜¯ sa2 batch {B_pc} çš„æ•´æ•°å€"
        L = B_L // B_pc   # æ¯ä¸ªåœºæ™¯çš„å¥å­æ•°
        D = self.lang_hidden_size

        # -------- æŠ•å½± SA2 ä¸ºä¸ lang_dim ç›¸åŒ --------
        # (B_pc, Np, Cpc) -> (B_pc, Np, D)
        sa2_proj = self.pc_proj(sa2)

        # -------- æ¯æ¡å¥å­å¤åˆ¶ SA2 patch --------
        # (B_pc, Np, D) -> (B_pc, L, Np, D)
        sa2_proj = sa2_proj.unsqueeze(1).repeat(1, L, 1, 1)
        # -> (B_pc*L, Np, D) == (B_L, Np, D)
        sa2_proj = sa2_proj.reshape(B_L, Np, D)

        # -------- Cross-Attention: token -> patch --------
        # ä¿å­˜åŸå§‹ BERT è¯­è¨€ç‰¹å¾
        orig_lang = lang_fea

        x = lang_fea  # (B_L, T, D)

        for layer in self.layers:
            residual = x
            attn_out, _ = layer["attn"](
                query=x,        # (B_L, T, D)
                key=sa2_proj,   # (B_L, Np, D)
                value=sa2_proj  # (B_L, Np, D)
            )
            x = layer["norm1"](residual + attn_out)

            residual = x
            ffn_out = layer["ffn"](x)
            x = layer["norm2"](residual + ffn_out)

        # -------- æ®‹å·®å¼èåˆï¼Œè€Œä¸æ˜¯ç›´æ¥è¦†ç›– --------
        gate = torch.sigmoid(self.gate_logit)  # åˆå§‹çº¦ 0.05
        # lang_new = åŸå§‹ + gate * (patch_fused - åŸå§‹)
        lang_new = orig_lang + gate * (x - orig_lang)

        data_dict["lang_fea"] = lang_new  # (B_L, T, D)
        return data_dict
